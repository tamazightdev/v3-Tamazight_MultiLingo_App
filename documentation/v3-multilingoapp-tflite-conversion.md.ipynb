{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Converting Fine-Tuned Gemma-3n Model to TFLite\n",
    "\n",
    "This notebook contains the complete process to:\n",
    "1. Install necessary libraries (including the latest Transformers from source).\n",
    "2. Authenticate with Hugging Face to download the model.\n",
    "3. Load your fine-tuned Gemma-3n model and tokenizer.\n",
    "4. Convert the model to the `.tflite` format using `ai-edge-torch`.\n",
    "5. Save the converted model and tokenizer assets for use in the Android app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install all required libraries\n",
    "# We install transformers directly from the GitHub main branch to get support for the newest models like Gemma-3n.\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install torch accelerate bitsandbytes sentencepiece ai-edge-torch --quiet\n",
    "\n",
    "print(\"✅ Libraries installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Authenticate with Hugging Face (Kaggle Version)\n",
    "# This cell uses the correct method for Kaggle Secrets.\n",
    "\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "    login(token=hf_token)\n",
    "    print(\"✅ Successfully authenticated with Hugging Face.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not log in. Please ensure you have attached the 'HF_TOKEN' secret via the 'Add-ons' menu. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Run the Full Conversion Script\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import ai_edge_torch\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "HF_MODEL_PATH = \"tamazightdev/v2-gemma-3n-4b-tmz-ft-vllm-merged\"\n",
    "OUTPUT_TFLITE_MODEL = \"gemma-3n-4b-tamazight-ft.tflite\"\n",
    "TOKENIZER_OUTPUT_DIR = './tokenizer_assets'\n",
    "\n",
    "print(\"--- Starting Model Conversion Process ---\")\n",
    "\n",
    "# --- 1. Load Hugging Face Model and Tokenizer ---\n",
    "print(f\"Loading model and tokenizer from: {HF_MODEL_PATH}\")\n",
    "try:\n",
    "    # Added trust_remote_code=True which is necessary for brand new architectures\n",
    "    model = AutoModelForCausalLM.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
    "    print(\"✅ Model and tokenizer loaded.\")\n",
    "\n",
    "    # It is critical to save the tokenizer's vocabulary for the app.\n",
    "    os.makedirs(TOKENIZER_OUTPUT_DIR, exist_ok=True)\n",
    "    tokenizer.save_pretrained(TOKENIZER_OUTPUT_DIR)\n",
    "    print(f\"✅ Tokenizer assets saved to '{TOKENIZER_OUTPUT_DIR}' directory.\")\n",
    "\n",
    "    # --- 2. Define an Example Input ---\n",
    "    # This helps the converter understand the model's architecture.\n",
    "    print(\"Defining an example input for the converter...\")\n",
    "    example_input_text = \"Translate to Tamazight: Hello, how are you?\"\n",
    "    inputs = tokenizer(example_input_text, return_tensors=\"pt\")\n",
    "    example_input = (inputs.input_ids,) # The converter expects a tuple of inputs\n",
    "    print(\"✅ Example input created.\")\n",
    "\n",
    "    # --- 3. Convert the Model to TFLite ---\n",
    "    print(\"Converting model to TFLite format with INT8 quantization...\")\n",
    "    edge_model = ai_edge_torch.convert(\n",
    "        model,\n",
    "        example_input,\n",
    "        quantization_type=ai_edge_torch.quantization.QuantizationType.INT8\n",
    "    )\n",
    "    print(\"✅ Model successfully converted.\")\n",
    "\n",
    "    # --- 4. Save the Converted Model ---\n",
    "    with open(OUTPUT_TFLITE_MODEL, \"wb\") as f:\n",
    "        f.write(edge_model)\n",
    "\n",
    "    print(\"\\n--- Conversion Complete! ---\")\n",
    "    print(f\"✅ Successfully saved TFLite model to: {OUTPUT_TFLITE_MODEL}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- An Error Occurred ---\")\n",
    "    print(f\"Error during conversion: {e}\")\n",
    "    print(\"Please check the model path, your Hugging Face token permissions, and available RAM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "After running the cells above, you should have two main outputs in your Kaggle environment's file system (check the file panel on the left):\n",
    "\n",
    "1.  **`gemma-3n-4b-tamazight-ft.tflite`**: This is your quantized, on-device model.\n",
    "2.  A folder named **`tokenizer_assets`**: This contains `tokenizer.json` and other necessary files.\n",
    "\n",
    "You will need to **download both the `.tflite` file and the `tokenizer.json` file** to your local machine to integrate them into your Android project as described in Stage 2 of the instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}