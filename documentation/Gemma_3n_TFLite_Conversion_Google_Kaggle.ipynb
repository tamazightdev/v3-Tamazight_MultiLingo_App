{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-markdown",
   "metadata": {},
   "source": [
    "# Gemma_3n_TFLite_Conversion\n",
    "# You must use a L100 or A100 GPU\n",
    "This notebook has been updated to fix the installation error and implement the robust conversion method required for Gemma-3n models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-libs-fixed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install all required modern libraries\n",
    "# We install transformers directly from the GitHub main branch for the newest models.\n",
    "!pip install --upgrade pip\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "\n",
    "# Install the correct 'ai-edge-torch' package and other essentials.\n",
    "!pip install --upgrade torch accelerate bitsandbytes sentencepiece \"ai-edge-torch>=0.2.1\" timm\n",
    "\n",
    "print(\"✅ Libraries installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auth-hf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Authenticate with Hugging Face\n",
    "# Make sure to add your Hugging Face token to Kaggle's \"Secrets\" with the name HF_TOKEN.\n",
    "from huggingface_hub import notebook_login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "try:\n",
    "    uc = UserSecretsClient()\n",
    "    hf_token = uc.get_secret(\"HF_TOKEN\")\n",
    "    notebook_login(token=hf_token)\n",
    "    print(\"✅ Successfully authenticated with Hugging Face.\")\n",
    "except Exception as e:\n",
    "    print(f\"Authentication failed: {e}\")\n",
    "    print(\"Please ensure your HF_TOKEN is correctly set in Kaggle Secrets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conversion-code-fixed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load Model, Convert, and Save (with Wrapper)\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, Gemma3nForConditionalGeneration\n",
    "import ai_edge_torch\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_ID = \"tamazightdev/v2-gemma-3n-4b-tmz-ft-vllm-merged\"\n",
    "OUTPUT_TFLITE_MODEL = \"gemma-3n-4b-tamazight-ft.tflite\"\n",
    "TOKENIZER_ASSETS_DIR = \"tokenizer_assets\"\n",
    "\n",
    "print(f\"--- Starting conversion for model: {MODEL_ID} ---\")\n",
    "\n",
    "# --- Define the Traceable Wrapper ---\n",
    "class Gemma3nForTFLite(torch.nn.Module):\n",
    "    \"\"\"A traceable wrapper for Gemma 3n for single-step autoregressive decoding.\"\"\"\n",
    "    def __init__(self, model_path: str):\n",
    "        super().__init__()\n",
    "        print(f\"Loading model from {model_path}...\")\n",
    "        self.model = Gemma3nForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float32 # Load in FP32 for stable conversion\n",
    "        ).eval()\n",
    "        print(\"✅ Model loaded successfully into wrapper.\")\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        \"\"\"Performs a single forward pass to get the next token logits.\"\"\"\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=False\n",
    "        )\n",
    "        # Return logits for the last token in the sequence [batch_size, vocab_size]\n",
    "        return outputs.logits[:, -1, :]\n",
    "\n",
    "try:\n",
    "    # 1. Load the tokenizer and the wrapped model\n",
    "    print(\"\\n1. Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    traceable_model = Gemma3nForTFLite(MODEL_ID)\n",
    "    print(\"✅ Tokenizer and wrapped model loaded.\")\n",
    "\n",
    "    # 2. Prepare an example input for the converter to trace the model's graph.\n",
    "    print(\"\\n2. Preparing example input for tracing...\")\n",
    "    # The wrapper's forward() method expects both input_ids and attention_mask.\n",
    "    sample_input_ids = torch.randint(0, 32000, (1, 128), dtype=torch.long)\n",
    "    sample_attention_mask = torch.ones((1, 128), dtype=torch.long)\n",
    "    sample_inputs = (sample_input_ids, sample_attention_mask)\n",
    "    print(\"✅ Example input prepared.\")\n",
    "\n",
    "    # 3. Convert the wrapped model to TFLite format\n",
    "    print(f\"\\n3. Converting model to TFLite format...\")\n",
    "    edge_model_bytes = ai_edge_torch.convert(\n",
    "        traceable_model,\n",
    "        sample_inputs\n",
    "    )\n",
    "    print(\"✅ Model successfully converted.\")\n",
    "\n",
    "    # 4. Save the TFLite model to a file\n",
    "    print(f\"\\n4. Saving TFLite model to {OUTPUT_TFLITE_MODEL}...\")\n",
    "    with open(OUTPUT_TFLITE_MODEL, \"wb\") as f:\n",
    "        f.write(edge_model_bytes)\n",
    "    print(\"✅ TFLite model saved.\")\n",
    "    \n",
    "    # 5. Save the tokenizer assets for your Android application\n",
    "    print(f\"\\n5. Saving tokenizer assets to {TOKENIZER_ASSETS_DIR}...\")\n",
    "    if not os.path.exists(TOKENIZER_ASSETS_DIR):\n",
    "        os.makedirs(TOKENIZER_ASSETS_DIR)\n",
    "    tokenizer.save_pretrained(TOKENIZER_ASSETS_DIR)\n",
    "    print(f\"✅ Tokenizer assets saved.\")\n",
    "    \n",
    "    print(\"\\n--- Conversion Complete! ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"\\n--- An Error Occurred ---\")\n",
    "    print(f\"Error during conversion: {e}\")\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nPlease check the model path, your Hugging Face token permissions, and available RAM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "After running the cells above, you should have your converted assets ready in the Kaggle file system (check the file panel on the right):\n",
    "\n",
    "1.  **`gemma-3n-4b-tamazight-ft.tflite`**: This is your on-device model.\n",
    "2.  A folder named **`tokenizer_assets`**: This contains `tokenizer.json` and other necessary files for your app.\n",
    "\n",
    "You will need to **download both the `.tflite` file and the `tokenizer.json` file** to integrate them into your Android project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
